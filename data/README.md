# TaxDown data engineer challenge

## About the position

You will be building the data pipelines that power TaxDown. Working shoulder to shoulder with the rest of the Tech team, Product and Marketing. Data is at the core of our business and we need to make sure it is prepared to scale.

We are looking for someone who can help us grow by contributing their experience and their desire to do things well. Designing architectures, solving problems, giving ideas and, of course, developing quality software are the things you will do here.

The main objective is that the whole team can learn from you. And of course, we hope that you can learn a lot from us. It is always important to grow while working and we want you to become more senior than you already are with us (the real "Win - Win" ðŸš€).

## Take me to the challenge! ðŸ¤Ÿ

One of our responsabilities as the Data team is to keep track of the online ads campaigns that the marketing team runs so we can then aggregate the cost, clicks, impressions, etc generated by those campaigns.

In this challenge (don't worry we don't want you to do work for free, we'll keep it short), you will have to implement a tool that is able to parse log files containing the online visits and gather a couple of metrics based on aggregates.

The idea is to use **Python** which is the language we use in the team, any other decision is your own although we encourage you to NOT use too many external libraries. Also do not use any external databases or cache such as Redis.

The only libraries we would like you to use are:
- Anything related with testing the solution.
- Typing, linting or styling, such as mypy, flake8, black, etc, although they are not required.
- Dependency management such as pypi or poetry.

ðŸ‘‚ *Everything you consider "Best Practices" is mandatory to implement ("We **LOVE** testing")*

### The log file ðŸŒŸ

Consider a log file that contains the following fields in csv format:

- timestamp (unix time stamp)
- anonymous_id (string: 8f80bc6a-7ecc-4709-b71e-03c2f76d3387)
- utm_medium (string: social/tv/partners)
- utm_source (string: twitter/google/meta)
- utm_campaign (string: early_buyers)

        1667233287,75c738cf-0841-42cf-939a-67e5f3a86325,social,meta,ig_followers
        1667234389,6ea9ab41-0018-4077-9053-ee87a27de999,partners,santander,lead
        1667235690,8f80bc6a-7ecc-4709-b71e-03c2f76d3387,social,google,early_buyers
        ...

The file is sorted by timestamp so you can ignore any row that is out of order.
It may be that the file has been written and closed or is still being written and run indefinitely.
Keep in mind that this file can be very long as we get millions of visitors during the tax season so your solution should be very efficient in both CPU and memory.

### First step ðŸŒŸ

For every hour display:
- Campaign with the most visits
- Source with the most visits
- Medium with the most visits

### Second step

After running the tool for a few weeks we realize that there are a few anonymous_ids visiting our website consistently and we suspect that we are receiving some kind of attack or that an advertiser may be trying to scam us with fake visits.
We have decided that if the same anonymous_id visit us more than 10 times in a window of five minutes we will ban that anonymous id for the next hour and don't count those visits towards the hourly aggregates.

Please 
ðŸ‘‚ *Psst, remember to include instructions on how to run your code, tests, etc*

### Third step

Prepare to be asked a lot of questions on your choices ðŸ˜†

And last but not least, it would be great if you add tests. Easy and simple no? **So we are done!**

## How can I share my solution? ðŸ”¥

I guess you used Git all the way here and made a few commits already, so how about creating a private repo and inviting us [Alvaro Correa](https://github.com/corrius)

This way, we can review your code and have it at hand for the next step, a personal interview! ðŸ‘»

Good luck with the challenge! Enjoy it and do your best!
